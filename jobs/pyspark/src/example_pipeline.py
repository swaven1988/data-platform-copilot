"""
COPILOT-GENERATED
job_name: example_pipeline
source: raw_db.source_table
target: curated_db.target_table
partition_column: data_dt
write_mode: merge
"""

import argparse
import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F


def parse_args():
    p = argparse.ArgumentParser(description="Example Spark pipeline generated by Copilot")
    p.add_argument("--env", required=True, help="Environment name (dev/qa/prod)")
    p.add_argument("--run_date", required=False, help="Run date override (YYYY-MM-DD)")
    return p.parse_args()


def main():
    args = parse_args()
    spark = (
        SparkSession.builder
        .appName("example_pipeline")
        .getOrCreate()
    )

    try:
        # Example read
        df = spark.table("raw_db.source_table")

        # TODO: Replace with real transformation logic
        df_out = df.withColumn("copilot_run_ts", F.current_timestamp())

        # Example write patterns
        # merge/upsert is typically handled via SQL MERGE INTO for Iceberg
        df_out.createOrReplaceTempView("staging_example_pipeline")
        spark.sql("""
        MERGE INTO curated_db.target_table t
        USING staging_example_pipeline s
        ON t.data_dt = s.data_dt
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
        """)

        spark.stop()
        return 0
    except Exception as e:
        print(f"[ERROR] Job failed: {e}", file=sys.stderr)
        spark.stop()
        return 1


if __name__ == "__main__":
    raise SystemExit(main())