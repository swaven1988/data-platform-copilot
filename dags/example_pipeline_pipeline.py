"""
COPILOT-GENERATED AIRFLOW DAG
job_name: example_pipeline
schedule: 0 6 * * *
timezone: UTC
"""

from __future__ import annotations

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator


DEFAULT_ARGS = {
    "owner": "data-platform",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=10),
}


with DAG(
    dag_id="example_pipeline_pipeline",
    default_args=DEFAULT_ARGS,
    description="Example Spark pipeline generated by Copilot",
    schedule="0 6 * * *",
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["copilot", "dev"],
) as dag:

    run_pyspark_job = BashOperator(
        task_id="run_pyspark_job",
        bash_command=(
            "spark-submit "
            "--conf spark.app.name=example_pipeline "
            "jobs/pyspark/src/example_pipeline.py "
            "--env dev"
        ),
    )