from __future__ import annotations

from typing import Dict

from app.core.spec_schema import CopilotSpec


def generate_pyspark_job(spec: CopilotSpec) -> Dict[str, str]:
    """
    Canonical PySpark job generator.

    Returns:
      {"jobs/pyspark/src/<job_name>.py": "<python code>"}
    """
    job_path = f"jobs/pyspark/src/{spec.job_name}.py"
    partition_col = spec.partition_column

    # ------------------------------------------------------------------
    # Write logic
    # ------------------------------------------------------------------
    if spec.write_mode == "merge":
        write_block = f"""
        df_out.createOrReplaceTempView("staging_{spec.job_name}")

        # Generic MERGE template (Iceberg/Delta style).
        spark.sql(\"\"\"
        MERGE INTO {spec.target_table} t
        USING staging_{spec.job_name} s
        ON t.{partition_col} = s.{partition_col}
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
        \"\"\")
"""
    elif spec.write_mode == "append":
        write_block = f"""
        (
            df_out.write
            .mode("append")
            .format("parquet")
            .insertInto("{spec.target_table}", overwrite=False)
        )
"""
    elif spec.write_mode == "overwrite":
        write_block = f"""
        (
            df_out.write
            .mode("overwrite")
            .format("parquet")
            .insertInto("{spec.target_table}", overwrite=True)
        )
"""
    else:
        raise ValueError(f"Unsupported write_mode: {spec.write_mode}")

    # ------------------------------------------------------------------
    # Spark job template
    # ------------------------------------------------------------------
    code = f"""\
\"\"\"
COPILOT-GENERATED SPARK JOB
job_name: {spec.job_name}
preset: {spec.preset}
source: {spec.source_table}
target: {spec.target_table}
partition_column: {spec.partition_column}
write_mode: {spec.write_mode}
\"\"\"

import argparse
import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F


def parse_args():
    p = argparse.ArgumentParser(description="Spark job generated by Copilot")
    p.add_argument("--env", required=True, help="Environment name (dev/qa/prod)")
    p.add_argument("--run_date", required=False, help="Run date override (YYYY-MM-DD)")
    return p.parse_args()


def main():
    args = parse_args()
    spark = SparkSession.builder.appName({spec.job_name!r}).getOrCreate()

    try:
        df = spark.table({spec.source_table!r})

        # TODO: Replace with real transformation logic
        df_out = df.withColumn("copilot_run_ts", F.current_timestamp())
{write_block.rstrip()}

        spark.stop()
        return 0

    except Exception as e:
        print(f"[ERROR] Job failed: {{e}}", file=sys.stderr)
        try:
            spark.stop()
        except Exception:
            pass
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
"""

    return {job_path: code}


# -------------------------------------------------------------------
# Stable public API surface (V1 + V2)
# -------------------------------------------------------------------
def render_pyspark_job(spec: CopilotSpec) -> Dict[str, str]:
    """Build V1 compatibility entrypoint."""
    return generate_pyspark_job(spec)


def render_pyspark_job_v2(spec: CopilotSpec) -> Dict[str, str]:
    """Build V2 compatibility entrypoint."""
    return generate_pyspark_job(spec)
