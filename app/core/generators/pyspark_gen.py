from __future__ import annotations

from typing import Dict

from app.core.spec_schema import CopilotSpec


# ---------------------------------------------------------------------------
# Preset transformation catalog
# Each value is a Python expression string that produces `df_out` from `df`.
# Available locals inside the generated job: spark, df, F, args, partition_col.
# ---------------------------------------------------------------------------
_PRESET_CATALOG: Dict[str, str] = {
    # ── legacy/default presets (explicit passthrough) ───────────────────────
    "generic": "df_out = df",
    "enterprise_a": "df_out = df",
    "enterprise_b": "df_out = df",

    # ── passthrough ─────────────────────────────────────────────────────────
    "copy": "df_out = df",

    # ── null-partition filter ────────────────────────────────────────────────
    "filter": "df_out = df.filter(F.col(partition_col).isNotNull())",

    # ── deduplication on partition column ───────────────────────────────────
    "dedup": "df_out = df.dropDuplicates([partition_col])",

    # ── safe string-cast (coerce all columns to StringType) ─────────────────
    "cast": (
        "from pyspark.sql.types import StringType\n"
        "df_out = df.select([F.col(c).cast(StringType()).alias(c) for c in df.columns])"
    ),

    # ── left-join with a lookup table (lookup_table from spec metadata) ─────
    "join": (
        "lookup_table = spec_meta.get('lookup_table', '')\n"
        "join_key    = spec_meta.get('join_key', partition_col)\n"
        "if lookup_table:\n"
        "    df_lookup = spark.table(lookup_table)\n"
        "    df_out = df.join(df_lookup, on=join_key, how='left')\n"
        "else:\n"
        "    df_out = df"
    ),
}

def _build_transform_block(preset: str) -> str:
    """Return the indented transformation lines for the given preset."""
    transform = _PRESET_CATALOG.get(preset)
    if transform is None:
        transform = (
            f"# PRESET_NOT_IMPLEMENTED: {preset!r}; using passthrough fallback\n"
            "df_out = df"
        )
    # Indent each line by 8 spaces (inside main())
    lines = transform.splitlines()
    return "\n".join(f"        {ln}" for ln in lines)


def generate_pyspark_job(spec: CopilotSpec) -> Dict[str, str]:
    """
    Canonical PySpark job generator.

    Returns:
      {"jobs/pyspark/src/<job_name>.py": "<python code>"}
    """
    job_path = f"jobs/pyspark/src/{spec.job_name}.py"
    partition_col = spec.partition_column

    # ------------------------------------------------------------------
    # Write logic
    # ------------------------------------------------------------------
    if spec.write_mode == "merge":
        write_block = f"""
        df_out.createOrReplaceTempView("staging_{spec.job_name}")

        # Generic MERGE template (Iceberg/Delta style).
        spark.sql(\"\"\"\
        MERGE INTO {spec.target_table} t
        USING staging_{spec.job_name} s
        ON t.{partition_col} = s.{partition_col}
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
        \"\"\")
"""
    elif spec.write_mode == "append":
        write_block = f"""
        (
            df_out.write
            .mode("append")
            .format("parquet")
            .insertInto("{spec.target_table}", overwrite=False)
        )
"""
    elif spec.write_mode == "overwrite":
        write_block = f"""
        (
            df_out.write
            .mode("overwrite")
            .format("parquet")
            .insertInto("{spec.target_table}", overwrite=True)
        )
"""
    else:
        raise ValueError(f"Unsupported write_mode: {spec.write_mode}")

    # ------------------------------------------------------------------
    # Transformation block
    # ------------------------------------------------------------------
    transform_block = _build_transform_block(spec.preset)

    # ------------------------------------------------------------------
    # Spark job template
    # ------------------------------------------------------------------
    code = f"""\
\"\"\"
COPILOT-GENERATED SPARK JOB
job_name: {spec.job_name}
preset: {spec.preset}
source: {spec.source_table}
target: {spec.target_table}
partition_column: {spec.partition_column}
write_mode: {spec.write_mode}
\"\"\"

import argparse
import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F


def parse_args():
    p = argparse.ArgumentParser(description="Spark job generated by Copilot")
    p.add_argument("--env", required=True, help="Environment name (dev/qa/prod)")
    p.add_argument("--run_date", required=False, help="Run date override (YYYY-MM-DD)")
    return p.parse_args()


def main():
    args = parse_args()
    spark = SparkSession.builder.appName({spec.job_name!r}).getOrCreate()

    # Spec metadata available for join preset etc.
    spec_meta = {getattr(spec, 'metadata', {})!r}  # type: ignore[assignment]  # noqa: F841
    partition_col = {partition_col!r}

    try:
        df = spark.table({spec.source_table!r})

        # ── Preset: {spec.preset!r} ──
{transform_block}
{write_block.rstrip()}

        spark.stop()
        return 0

    except Exception as e:
        print(f"[ERROR] Job failed: {{e}}", file=sys.stderr)
        try:
            spark.stop()
        except Exception:
            pass
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
"""

    return {job_path: code}


# -------------------------------------------------------------------
# Stable public API surface (V1 + V2)
# -------------------------------------------------------------------
def render_pyspark_job(spec: CopilotSpec) -> Dict[str, str]:
    """Build V1 compatibility entrypoint."""
    return generate_pyspark_job(spec)


def render_pyspark_job_v2(spec: CopilotSpec) -> Dict[str, str]:
    """Build V2 compatibility entrypoint."""
    return generate_pyspark_job(spec)
